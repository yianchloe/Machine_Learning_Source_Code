{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NLP Embedding Sentence Transformer"
      ],
      "metadata": {
        "id": "UgyI4sKFZmhm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_De2bVmzZjcg"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch.quantization import quantize_dynamic\n",
        "model_st = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "model_qt = quantize_dynamic(model_st)\n",
        "#embeddings = model_qt.encode(utterance_text_ls)\n",
        "#embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Embeddings to dataframe"
      ],
      "metadata": {
        "id": "Av_ItY-paHYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_columns = [f'Embedding_{i}' for i in range(hashtag_embedding.shape[1])]\n",
        "df[embedding_columns] = hashtag_embedding"
      ],
      "metadata": {
        "id": "fb5zZMEdZ_rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fasttext"
      ],
      "metadata": {
        "id": "VD7JW-B-pWzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
        "model = fasttext.load_model(model_path)\n",
        "model.predict(\"Hello, world!\")"
      ],
      "metadata": {
        "id": "Z83BkynTpX9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.load_model(model_path)\n",
        "model = fasttext.train_supervised(input=\"formatted_data.txt\", maxn = 9)\n",
        "model.save_model(\"model_ft.bin\")"
      ],
      "metadata": {
        "id": "8NTkGFgypdMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output file path\n",
        "output_file = 'formatted_data.txt'\n",
        "\n",
        "# Open the file for writing\n",
        "with open(output_file, 'w') as file:\n",
        "    # Iterate through DataFrame rows\n",
        "    for index, row in df.iterrows():\n",
        "        # Format Column1 and Column2 as \"__label__word1-word2\"\n",
        "        formatted_column1 = \"__label__\" + row[\"reorg_issue\"].replace(\" \", \"-\")\n",
        "        formatted_column2 = \"__label__\" + row[\"reorg_sub_issue\"].replace(\" \", \"-\")\n",
        "\n",
        "        # Column3 is concatenated as is\n",
        "        formatted_column3 = row[\"challenge_name\"]\n",
        "\n",
        "        # Combine all formatted columns and write to the file\n",
        "        formatted_row = f'{formatted_column1} {formatted_column2} {formatted_column3}\\n'\n",
        "        file.write(formatted_row)\n",
        "\n",
        "print(f\"Formatted data saved to {output_file}\")"
      ],
      "metadata": {
        "id": "H_u_6bNwpiAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = 'formatted_data.txt'\n",
        "with open(output_file, 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "NE1tHxY9pi1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinal Encoding for categorical variable"
      ],
      "metadata": {
        "id": "2SclkJpCZ7JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the categorical data\n",
        "vv_mapping = {k: i for i, k in enumerate(\n",
        "   [\"(0, 1K]\",\"(1K, 10K]\",\"(10K, 100K]\",\"(100K, 1M]\",\"(1M, 10M]\",\"10M+\"], 0)}\n",
        "df.video_vv_strata = df.video_vv_strata.map(vv_mapping)\n",
        "publish_mapping = {k: i for i, k in enumerate(\n",
        "   ['(0, 20]','(20, 100]','(100, 1K]','(1K, 10K]','(10K, 100K]','100K+'], 0)}\n",
        "df.video_publish_strata = df.video_publish_strata.map(publish_mapping)\n",
        "df.top1_video_publish_coutnry_code_15d = df.top1_video_publish_coutnry_code_15d.astype('category')"
      ],
      "metadata": {
        "id": "_L-x-Vn7Z12I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encoding"
      ],
      "metadata": {
        "id": "ymiXwVmBprQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Define the column to be one-hot encoded\\n\",\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "categorical_column = ['top1_video_publish_coutnry_code_15d']  # Name of the column to be encoded\\n\",\n",
        "\n",
        "    # Define a function to extract the feature names\\n\",\n",
        "def get_feature_names(ohe, input_features):\n",
        "        return ohe.get_feature_names_out(input_features)\n",
        "\n",
        "    # Initialize the ColumnTransformer\\n\",\n",
        "transformer = ColumnTransformer(\n",
        "       transformers=[\n",
        "      ('onehot', OneHotEncoder(), categorical_column)],remainder='passthrough')  # Pass through other columns unchanged\\n\",\n",
        "\n",
        "    # Fit and transform the data\\n\",\n",
        "encoded_data = transformer.fit_transform(hashtag_clustering)\n",
        "    # Get the transformed feature names\\n\",\n",
        "feature_names = get_feature_names(transformer.named_transformers_['onehot'], categorical_column)\n",
        "    # Create a DataFrame with the transformed data and feature names\",\n",
        "#client = pd.DataFrame(encoded_data, columns=list(feature_names) + list(client.columns[1:]))\","
      ],
      "metadata": {
        "id": "FZb_XJqEpthb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardizing data"
      ],
      "metadata": {
        "id": "wmRvCb_vaOPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "#scaled_data = scaler.fit_transform(df.iloc[:,list(range(9,16))])\n",
        "scaled_data = scaler.fit_transform(hashtag_clustering.iloc[:,1:-1])\n",
        "hashtag_clustering.iloc[:,1:-1] = scaled_data"
      ],
      "metadata": {
        "id": "SBr2PfkGaQ5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram"
      ],
      "metadata": {
        "id": "EvA31ZiroPKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a histogram\n",
        "plt.hist(violation_rate['violation_rate_15d'], bins=5, color='skyblue', edgecolor='black')  # Adjust the number of bins as needed\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram Plot')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RqtgNiekoQS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K means clustering"
      ],
      "metadata": {
        "id": "QFkzooeIaY_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a K-Means model with the desired number of clusters (k)\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "\n",
        "# Fit the model to your numerical data\n",
        "kmeans.fit(scaled_data)\n",
        "\n",
        "# Get cluster assignments for each data point\n",
        "cluster_labels = kmeans.labels_"
      ],
      "metadata": {
        "id": "LeWE1WtBabUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use silouette plot and silouette score to determine what is the best number of cluster, the higher the better"
      ],
      "metadata": {
        "id": "I2zoTKcAgntr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "X = scaled_data\n",
        "\n",
        "\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\n",
        "        \"For n_clusters =\",\n",
        "        n_clusters,\n",
        "        \"The average silhouette_score is :\",\n",
        "        silhouette_avg,\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(\n",
        "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
        "    )\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(\n",
        "        centers[:, 0],\n",
        "        centers[:, 1],\n",
        "        marker=\"o\",\n",
        "        c=\"white\",\n",
        "        alpha=1,\n",
        "        s=200,\n",
        "        edgecolor=\"k\",\n",
        "    )\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
        "        % n_clusters,\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TE-u7LTZghkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierachical clustering\n",
        "\n",
        "For dendrogram, have a horizontal line as the threshold that cuts off, each intersection is a cluster. The y axis is the distance apart in between each cluster."
      ],
      "metadata": {
        "id": "gfB5gGUjg2vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "linkage_matrix = linkage(scaled_data, method='ward')  # You can choose different linkage methods\n",
        "# Plot the dendrogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linkage_matrix, leaf_rotation=90, leaf_font_size=12)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# Choose a threshold or number of clusters (e.g., by visually inspecting the dendrogram)\n",
        "threshold = 50  # Adjust this value based on your dendrogram\n",
        "\n",
        "# Extract clusters based on the threshold\n",
        "clusters = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
        "\n",
        "# Assign cluster labels to data points\n",
        "your_data['Cluster'] = clusters\n"
      ],
      "metadata": {
        "id": "IJAd3SpFg5ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isolation Forest"
      ],
      "metadata": {
        "id": "3os3JmZNjBWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "# Visualize the results (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# Create and fit the Isolation Forest model\n",
        "model = IsolationForest(contamination=0.1)  # Contamination parameter defines the expected proportion of anomalies\n",
        "model.fit(hashtag_clustering.iloc[:,0:-1])\n",
        "\n",
        "# Predict outliers (1 for inliers, -1 for outliers)\n",
        "y_pred = model.predict(hashtag_clustering.iloc[:,0:-1])\n",
        "\n",
        "# Apply PCA to reduce the data to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(hashtag_clustering.iloc[:,0:-1])\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Isolation Forest Anomaly Detection (PCA Visualization)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AvC0FdaUjDfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Learning"
      ],
      "metadata": {
        "id": "yASR_IiOk8YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling on the imbalanced dataset"
      ],
      "metadata": {
        "id": "iWLOA0YulmNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "opO1je9Qk-aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minority_class = True\n",
        "majority_class = False\n",
        "\n",
        "# Count the number of instances in the minority class\n",
        "minority_count = np.sum(y == minority_class)\n",
        "\n",
        "# Count the number of instances in the majority class\n",
        "majority_count = np.sum(y != minority_class)\n",
        "\n",
        "# Calculate the oversampling ratio\n",
        "oversampling_ratio = int(majority_count / minority_count)\n",
        "\n",
        "# Indices of instances in the minority class\n",
        "minority_indices = np.where(y == minority_class)[0]\n",
        "\n",
        "# Oversample the minority class\n",
        "oversampled_X = X[minority_indices]\n",
        "oversampled_y = y[minority_indices]\n",
        "\n",
        "for _ in range(oversampling_ratio - 1):\n",
        "    oversampled_X = np.vstack((oversampled_X, X[minority_indices]))\n",
        "    oversampled_y = np.hstack((oversampled_y, y[minority_indices]))\n",
        "\n",
        "# Combine oversampled data with the majority class\n",
        "X_resampled = np.vstack((X[y != minority_class], oversampled_X))\n",
        "y_resampled = np.hstack((y[y != minority_class], oversampled_y))\n",
        "\n",
        "# Now X_resampled and y_resampled contain the oversampled data"
      ],
      "metadata": {
        "id": "9IbpOabglKxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "xgboost"
      ],
      "metadata": {
        "id": "oKgs3fBJl6Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "X_train, y_train = X_resampled, y_resampled\n",
        "\n",
        "# Create a DMatrix for XGBoost (an internal data structure for efficient data handling)\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set hyperparameters for the XGBoost model\n",
        "params = {\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'objective': 'multi:softmax',  # Multiclass classification\n",
        "    'num_class': 3,  # Number of classes in your target variable\n",
        "}\n",
        "\n",
        "num_round = 100  # Number of boosting rounds\n",
        "\n",
        "# Train the XGBoost model\n",
        "model = xgb.train(params, dtrain, num_round)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(dtest)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "5cm3GkBRl5Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA"
      ],
      "metadata": {
        "id": "RraJf6QvmS9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to reduce the data to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(hashtag_clustering_.iloc[:,3:-1])\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "#scaled_data = scaler.fit_transform(df.iloc[:,list(range(9,16))])\n",
        "X_reduced = scaler.fit_transform(X_reduced)\n",
        "\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=hashtag_clustering_['high_risk'], cmap='viridis')\n",
        "\n",
        "# Add the legend\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar()\n",
        "plt.title('Isolation Forest Anomaly Detection (PCA Visualization)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BHEwQzsUmTL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}